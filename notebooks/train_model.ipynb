{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_QIhJsV0eyP"
   },
   "source": [
    "The purpose of this notebook is to use transfer learning in order to predict the Culture of the artwork from MET given images. In order to do that, we use GPU offered by Google Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTv5GpjH05EC"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lUdiHiy5_zc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoGqgJLk09lM"
   },
   "source": [
    "The dataset is stored in google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ckp_g2vVNd7",
    "outputId": "c7ce30e9-7b98-4607-d0b0-e8a2aa17b165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#get access to google colab\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lY6lU3IVimM"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset is stored in a zip file \n",
    "\"\"\"\n",
    "zip_file = '/content/gdrive/MyDrive/MUSEUM_IMAGES.zip' #path of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0XQ55ws1HVy"
   },
   "source": [
    "Run the following cell if and only if you have not yet unzip your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptqytLV0VbLy"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/gdrive/MyDrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt27wesS12wV"
   },
   "source": [
    "In the following cell, we create the train, validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UdbXOGe2WbNz",
    "outputId": "11133562-a389-41b6-d0a3-a1cf93c0413b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7153 files belonging to 9 classes.\n",
      "Using 5723 files for training.\n",
      "Found 7153 files belonging to 9 classes.\n",
      "Using 1430 files for validation.\n",
      "Found 1789 files belonging to 9 classes.\n",
      "Found 200 files belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "IMG_SIZE = (224, 224) #specific for VGG16\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "file_data = 'MUSEUM_IMAGES'\n",
    "data_dir_train = os.path.join('/content/gdrive/MyDrive', file_data, 'TRAIN')\n",
    "\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  image_size= IMG_SIZE,\n",
    "  seed=1,\n",
    "  batch_size= batch_size)\n",
    "\n",
    "\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=1,\n",
    "  image_size= IMG_SIZE,\n",
    "  batch_size= batch_size)\n",
    "\n",
    "\n",
    "\n",
    "data_dir_test_unbalanced = os.path.join('/content/gdrive/MyDrive',file_data, 'TEST_UNBALANCED')\n",
    "test_data_unbalanced = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir_test_unbalanced,\n",
    "  image_size= IMG_SIZE,\n",
    "  seed=1,\n",
    "  batch_size= batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bme2eYez1_F-"
   },
   "source": [
    "We import VGG16 in order to do transfer learning for our specific problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_XHeHZwW1IU",
    "outputId": "5f75cbae-10a5-44dd-a7a1-512103b754c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(input_shape=IMG_SHAPE , weights='imagenet', include_top=False)\n",
    "\n",
    "# don't train existing weights\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip21n3vbAo2w"
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
    "rescale = tf.keras.layers.Rescaling(scale=1./255)\n",
    "flatten = tf.keras.layers.Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QguVPGT_2Gwl"
   },
   "source": [
    "We build our model. We change the last dense layer by putting a 4-dense layer with a softmax as activation function in order to achieve our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8phSo6IW34_",
    "outputId": "d3246eb2-41e9-444d-b9af-636c7e3d5cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 9)                 225801    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,940,489\n",
      "Trainable params: 225,801\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 9\n",
    "x = Flatten()(base_model.output)\n",
    "prediction = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=base_model.input, outputs=prediction)\n",
    "\n",
    "# view the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kN1aXzUrIBM1",
    "outputId": "6cb23c66-9e3c-451a-abc0-8fdda09b8496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "90/90 [==============================] - 90s 830ms/step - loss: 174.5379 - accuracy: 0.3935 - val_loss: 178.4415 - val_accuracy: 0.4245\n",
      "Epoch 2/20\n",
      "90/90 [==============================] - 71s 764ms/step - loss: 61.5926 - accuracy: 0.6991 - val_loss: 204.1411 - val_accuracy: 0.4273\n",
      "Epoch 3/20\n",
      "90/90 [==============================] - 71s 766ms/step - loss: 33.7190 - accuracy: 0.8164 - val_loss: 209.5868 - val_accuracy: 0.4566\n",
      "Epoch 4/20\n",
      "90/90 [==============================] - 70s 761ms/step - loss: 23.0884 - accuracy: 0.8614 - val_loss: 225.5429 - val_accuracy: 0.4699\n",
      "Epoch 5/20\n",
      "90/90 [==============================] - 70s 761ms/step - loss: 14.9745 - accuracy: 0.8983 - val_loss: 234.6190 - val_accuracy: 0.4608\n",
      "Epoch 6/20\n",
      "90/90 [==============================] - 70s 759ms/step - loss: 10.9408 - accuracy: 0.9236 - val_loss: 269.4076 - val_accuracy: 0.4699\n",
      "Epoch 7/20\n",
      "90/90 [==============================] - 70s 760ms/step - loss: 9.9313 - accuracy: 0.9301 - val_loss: 262.1656 - val_accuracy: 0.4734\n",
      "Epoch 8/20\n",
      "90/90 [==============================] - 70s 754ms/step - loss: 10.8938 - accuracy: 0.9360 - val_loss: 268.5250 - val_accuracy: 0.4804\n",
      "Epoch 9/20\n",
      "90/90 [==============================] - 70s 754ms/step - loss: 9.8846 - accuracy: 0.9345 - val_loss: 300.4456 - val_accuracy: 0.4483\n",
      "Epoch 10/20\n",
      "90/90 [==============================] - 70s 755ms/step - loss: 10.6456 - accuracy: 0.9346 - val_loss: 309.3051 - val_accuracy: 0.4762\n",
      "Epoch 11/20\n",
      "90/90 [==============================] - 70s 757ms/step - loss: 9.0448 - accuracy: 0.9413 - val_loss: 324.3517 - val_accuracy: 0.4608\n",
      "Epoch 12/20\n",
      "90/90 [==============================] - 70s 759ms/step - loss: 8.5827 - accuracy: 0.9483 - val_loss: 331.3884 - val_accuracy: 0.4755\n",
      "Epoch 13/20\n",
      "90/90 [==============================] - 70s 759ms/step - loss: 7.2710 - accuracy: 0.9523 - val_loss: 327.8456 - val_accuracy: 0.4839\n",
      "Epoch 14/20\n",
      "90/90 [==============================] - 70s 761ms/step - loss: 6.2709 - accuracy: 0.9616 - val_loss: 368.4374 - val_accuracy: 0.4790\n",
      "Epoch 15/20\n",
      "90/90 [==============================] - 70s 760ms/step - loss: 6.5497 - accuracy: 0.9614 - val_loss: 355.0608 - val_accuracy: 0.4958\n",
      "Epoch 16/20\n",
      "90/90 [==============================] - 70s 759ms/step - loss: 5.9852 - accuracy: 0.9668 - val_loss: 395.4535 - val_accuracy: 0.4769\n",
      "Epoch 17/20\n",
      "90/90 [==============================] - 70s 759ms/step - loss: 9.9417 - accuracy: 0.9516 - val_loss: 408.1489 - val_accuracy: 0.4650\n",
      "Epoch 18/20\n",
      "90/90 [==============================] - 70s 755ms/step - loss: 6.9613 - accuracy: 0.9638 - val_loss: 421.1040 - val_accuracy: 0.4769\n",
      "Epoch 19/20\n",
      "90/90 [==============================] - 69s 753ms/step - loss: 7.3048 - accuracy: 0.9656 - val_loss: 402.5241 - val_accuracy: 0.4832\n",
      "Epoch 20/20\n",
      "90/90 [==============================] - 70s 757ms/step - loss: 7.2784 - accuracy: 0.9684 - val_loss: 430.8032 - val_accuracy: 0.4748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2349a62fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "model.compile(\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "model.fit( \n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx9AgV1V2WIA"
   },
   "source": [
    "The loss used is categorial cross entropy and the metric is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jrrO3-KfP1u"
   },
   "outputs": [],
   "source": [
    "#save the model in order to re-use the weights\n",
    "name_model = 'vgg16_culture_customized.h5'\n",
    "model.save(os.path.join('/content/gdrive/MyDrive',name_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXgqZZEE_JD_",
    "outputId": "2b2ccfc1-e4b2-4a36-b427-fdf3ee7988f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6200000047683716\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data_unbalanced, verbose=False)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Applied DL Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
